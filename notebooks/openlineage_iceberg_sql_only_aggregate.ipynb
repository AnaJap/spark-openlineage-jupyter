{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e3d2e2",
   "metadata": {},
   "source": [
    "# Iceberg SQL-Only Transform + Aggregate\n",
    "\n",
    "This notebook uses Spark SQL only (no DataFrame transformation API) to:\n",
    "- read from Iceberg table `local.bronze.marquez_raw_combined_v1`\n",
    "- apply modifications and aggregations\n",
    "- write results to a new Iceberg table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafbc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd96452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iceberg settings\n",
    "WAREHOUSE_PATH = \"/home/jovyan/work/data/lakehouse/warehouse\"\n",
    "SOURCE_TABLE = \"local.bronze.marquez_raw_combined_v1\"\n",
    "TARGET_TABLE = \"local.silver.marquez_sql_only_agg_v1\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"iceberg-sql-only-transform-aggregate\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", WAREHOUSE_PATH)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2beac375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|source_row_count|\n",
      "+----------------+\n",
      "|172             |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate source table exists\n",
    "spark.sql(f\"SELECT COUNT(*) AS source_row_count FROM {SOURCE_TABLE}\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df15a089-d5f5-4236-8b16-e6f1cb765f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- job_name: string (nullable = true)\n",
      " |-- job_namespace: string (nullable = true)\n",
      " |-- producer: string (nullable = true)\n",
      " |-- run_uuid: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- _event_type: string (nullable = true)\n",
      " |-- ingestion_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(SOURCE_TABLE).printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c38df6d5-d430-4226-a8d0-5d63f66f5463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {TARGET_TABLE}\n",
    "SELECT * FROM\n",
    "(WITH base AS (\n",
    "    SELECT\n",
    "        CAST(ingestion_ts AS TIMESTAMP) AS ingestion_ts,\n",
    "        TO_DATE(CAST(ingestion_ts AS TIMESTAMP)) AS ingestion_date,\n",
    "        CAST(COALESCE(event_type, event, job_name, job_namespace, producer) AS STRING) AS entity_value\n",
    "    FROM {SOURCE_TABLE}\n",
    "),\n",
    "cleaned AS (\n",
    "    SELECT\n",
    "        ingestion_ts,\n",
    "        ingestion_date,\n",
    "        UPPER(TRIM(entity_value)) AS entity_value_norm,\n",
    "        CASE\n",
    "            WHEN UPPER(TRIM(entity_value)) LIKE 'COMPLETE%' THEN 'COMPLETE'\n",
    "            WHEN UPPER(TRIM(entity_value)) LIKE 'START%' THEN 'START'\n",
    "            WHEN UPPER(TRIM(entity_value)) LIKE 'FAIL%' THEN 'FAIL'\n",
    "            ELSE 'OTHER'\n",
    "        END AS entity_bucket\n",
    "    FROM base\n",
    "    WHERE entity_value IS NOT NULL\n",
    "),\n",
    "agg AS (\n",
    "    SELECT\n",
    "        ingestion_date,\n",
    "        entity_bucket,\n",
    "        entity_value_norm,\n",
    "        COUNT(*) AS row_count,\n",
    "        MIN(ingestion_ts) AS first_seen_ts,\n",
    "        MAX(ingestion_ts) AS last_seen_ts\n",
    "    FROM cleaned\n",
    "    GROUP BY ingestion_date, entity_bucket, entity_value_norm\n",
    ")\n",
    "SELECT *\n",
    "FROM agg) A\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f57d9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL-only transformation + aggregation and save to Iceberg\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS local.silver\")\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE}\")\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "# CREATE TABLE {TARGET_TABLE}\n",
    "# USING iceberg\n",
    "# TBLPROPERTIES ('format-version'='2')\n",
    "# AS\n",
    "# WITH base AS (\n",
    "#     SELECT\n",
    "#         CAST(ingestion_ts AS TIMESTAMP) AS ingestion_ts,\n",
    "#         TO_DATE(CAST(ingestion_ts AS TIMESTAMP)) AS ingestion_date,\n",
    "#         CAST(COALESCE(event_type, event, job_name, job_namespace, producer) AS STRING) AS entity_value\n",
    "#     FROM {SOURCE_TABLE}\n",
    "# ),\n",
    "# cleaned AS (\n",
    "#     SELECT\n",
    "#         ingestion_ts,\n",
    "#         ingestion_date,\n",
    "#         UPPER(TRIM(entity_value)) AS entity_value_norm,\n",
    "#         CASE\n",
    "#             WHEN UPPER(TRIM(entity_value)) LIKE 'COMPLETE%' THEN 'COMPLETE'\n",
    "#             WHEN UPPER(TRIM(entity_value)) LIKE 'START%' THEN 'START'\n",
    "#             WHEN UPPER(TRIM(entity_value)) LIKE 'FAIL%' THEN 'FAIL'\n",
    "#             ELSE 'OTHER'\n",
    "#         END AS entity_bucket\n",
    "#     FROM base\n",
    "#     WHERE entity_value IS NOT NULL\n",
    "# ),\n",
    "# agg AS (\n",
    "#     SELECT\n",
    "#         ingestion_date,\n",
    "#         entity_bucket,\n",
    "#         entity_value_norm,\n",
    "#         COUNT(*) AS row_count,\n",
    "#         MIN(ingestion_ts) AS first_seen_ts,\n",
    "#         MAX(ingestion_ts) AS last_seen_ts\n",
    "#     FROM cleaned\n",
    "#     GROUP BY ingestion_date, entity_bucket, entity_value_norm\n",
    "# )\n",
    "# SELECT *\n",
    "# FROM agg\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2feebb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-----------------+---------+--------------------------+--------------------------+\n",
      "|ingestion_date|entity_bucket|entity_value_norm|row_count|first_seen_ts             |last_seen_ts              |\n",
      "+--------------+-------------+-----------------+---------+--------------------------+--------------------------+\n",
      "|2026-02-24    |OTHER        |RUNNING          |73       |2026-02-24 10:44:17.572374|2026-02-24 10:44:17.572374|\n",
      "|2026-02-24    |COMPLETE     |COMPLETE         |64       |2026-02-24 10:44:17.572374|2026-02-24 10:44:17.572374|\n",
      "|2026-02-24    |START        |START            |35       |2026-02-24 10:44:17.572374|2026-02-24 10:44:17.572374|\n",
      "+--------------+-------------+-----------------+---------+--------------------------+--------------------------+\n",
      "\n",
      "+----------------+\n",
      "|target_row_count|\n",
      "+----------------+\n",
      "|3               |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview written Iceberg table\n",
    "spark.sql(f\"SELECT * FROM {TARGET_TABLE} ORDER BY row_count DESC, ingestion_date, entity_value_norm LIMIT 100\").show(truncate=False)\n",
    "spark.sql(f\"SELECT COUNT(*) AS target_row_count FROM {TARGET_TABLE}\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e00f6",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- SQL operations are done through `spark.sql(...)` only.\n",
    "- Target table is unique and does not overwrite previous notebook outputs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
