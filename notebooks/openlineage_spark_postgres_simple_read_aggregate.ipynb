{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68276cda",
   "metadata": {},
   "source": [
    "# Spark Simple Read + Aggregate (Marquez Postgres)\n",
    "\n",
    "This notebook:\n",
    "- connects to the same Docker Postgres (`marquez` DB)\n",
    "- selects one random table from `public` schema\n",
    "- reads it with Spark JDBC\n",
    "- runs a small aggregation\n",
    "- saves the aggregation result as Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f45de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3ea091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Postgres connection settings (Docker Compose defaults)\n",
    "POSTGRES_HOST = \"postgres\"\n",
    "POSTGRES_PORT = \"5432\"\n",
    "POSTGRES_DB = \"marquez\"\n",
    "POSTGRES_USER = \"marquez\"\n",
    "POSTGRES_PASSWORD = \"marquez\"\n",
    "\n",
    "JDBC_URL = f\"jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "JDBC_PROPS = {\n",
    "    \"user\": POSTGRES_USER,\n",
    "    \"password\": POSTGRES_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"openlineage-postgres-simple-read-aggregate\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f55e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql(query: str):\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", JDBC_URL)\n",
    "        .option(\"driver\", JDBC_PROPS[\"driver\"])\n",
    "        .option(\"user\", JDBC_PROPS[\"user\"])\n",
    "        .option(\"password\", JDBC_PROPS[\"password\"])\n",
    "        .option(\"dbtable\", f\"({query}) as q\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "\n",
    "def read_table(table_name: str, schema: str = \"public\"):\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", JDBC_URL)\n",
    "        .option(\"driver\", JDBC_PROPS[\"driver\"])\n",
    "        .option(\"user\", JDBC_PROPS[\"user\"])\n",
    "        .option(\"password\", JDBC_PROPS[\"password\"])\n",
    "        .option(\"dbtable\", f\"{schema}.{table_name}\")\n",
    "        .load()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f80758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected table: jobs_tag_mapping\n"
     ]
    }
   ],
   "source": [
    "# Pick one random table from public schema (excluding migration history tables)\n",
    "random_table_df = read_sql(\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'public'\n",
    "      AND table_type = 'BASE TABLE'\n",
    "      AND table_name NOT ILIKE 'schema_version%'\n",
    "      AND table_name NOT ILIKE 'flyway%'\n",
    "    ORDER BY random()\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "row = random_table_df.first()\n",
    "if row is None:\n",
    "    raise RuntimeError(\"No eligible tables found in public schema\")\n",
    "\n",
    "selected_table = row[\"table_name\"]\n",
    "print(\"Selected table:\", selected_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7409790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source row count: 0\n",
      "root\n",
      " |-- job_uuid: string (nullable = true)\n",
      " |-- tag_uuid: string (nullable = true)\n",
      " |-- tagged_at: timestamp (nullable = true)\n",
      "\n",
      "+--------+--------+---------+\n",
      "|job_uuid|tag_uuid|tagged_at|\n",
      "+--------+--------+---------+\n",
      "+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df = read_table(selected_table)\n",
    "print(\"Source row count:\", source_df.count())\n",
    "source_df.printSchema()\n",
    "source_df.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10711cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping column used: job_uuid\n",
      "+-----------+---------+\n",
      "|group_value|row_count|\n",
      "+-----------+---------+\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small, generic aggregation: pick a grouping column and count rows\n",
    "preferred_group_cols = [\"namespace_name\", \"namespace\", \"type\", \"name\", \"event_type\", \"state\"]\n",
    "columns = source_df.columns\n",
    "\n",
    "group_col = None\n",
    "for c in preferred_group_cols:\n",
    "    if c in columns:\n",
    "        group_col = c\n",
    "        break\n",
    "\n",
    "if group_col is None and columns:\n",
    "    group_col = columns[0]\n",
    "\n",
    "if group_col is None:\n",
    "    raise RuntimeError(f\"Table {selected_table} has no columns\")\n",
    "\n",
    "agg_df = (\n",
    "    source_df\n",
    "    .groupBy(F.col(group_col).cast(\"string\").alias(\"group_value\"))\n",
    "    .agg(F.count(F.lit(1)).alias(\"row_count\"))\n",
    "    .orderBy(F.col(\"row_count\").desc(), F.col(\"group_value\"))\n",
    ")\n",
    "\n",
    "print(\"Grouping column used:\", group_col)\n",
    "agg_df.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47197cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet saved to: /home/jovyan/work/data/postgres_simple/jobs_tag_mapping_aggregation_parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/data/postgres_simple/jobs_tag_mapping_aggregation_parquet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save aggregated result as Parquet\n",
    "out_dir = \"/home/jovyan/work/data/postgres_simple\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "out_path = os.path.join(out_dir, f\"{selected_table}_aggregation_parquet\")\n",
    "agg_df.write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "print(\"Parquet saved to:\", out_path)\n",
    "out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb6bfc",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Run this from the Docker notebook container so host `postgres` resolves.\n",
    "- Re-running the notebook can pick a different random table.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
